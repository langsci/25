%% -*- coding:utf-8 -*-

\chapter[Universal Grammar and comparative linguistics without UG]{Universal Grammar and doing comparative linguistics without an a priori assumption of a (strong) UG}
\label{Abschnitt-UG-mit-Hierarchie}

The following two sections deal with the tools that I believe to be necessary to capture
generalizations and the way one can derive such generalizations.

\section{Formal tools for capturing generalizations}

In Chapter~\ref{chap-innateness}, it was shown that all the evidence that has previously been
brought forward in favor of innate linguistic knowledge is in fact controversial. In some cases, the
facts are irrelevant to the discussion and in other cases, they could be explained in other
ways. Sometimes, the chains of argumentation are not logically sound or the premises are not
supported. In other cases, the argumentation is circular. As a result, the question of whether there
is innate linguistic knowledge still remains unanswered. All theories that presuppose the existence
of this kind of knowledge are making very strong assumptions. If one assumes, as \citet{Kayne94a-u}
for example, that all languages have the underlying structure [specifier [head complement]] and that
movement is exclusively to the left, then
%, while it is possible to develop a very elegant system,
these two basic assumptions must be part of innate linguistic knowledge since there is no evidence for the
assumption that utterances in all natural languages have the structure that Kayne suggests. As an
example, the reader may check Laenzlinger's proposal for German \citeyearpar[\page 224]{Laenzlinger2004a}, which is depicted in
Figure~\ref{Abbildung-Remnant-Movement-Satzstruktur} on
page~\pageref{Abbildung-Remnant-Movement-Satzstruktur}. According to Laenzlinger, (\mex{1}a) is derived from the underlying
structure in (\mex{1}b):
\eal
\ex[]{
\gll weil der Mann wahrscheinlich diese Sonate nicht oft gut gespielt hat\hspace{-2pt}\\
     because the man probably this sonata not often well played has\hspace{-2pt}\\
\glt `because the man probably had not played this sonata well often'
}
\ex[*]{
\gll weil der Mann wahrscheinlich nicht oft gut hat gespielt diese Sonate\\
     because the man probably not often well has played this sonata\\ 
}
\zl
(\mex{0}b) is entirely unacceptable, so the respective structure cannot be acquired from input and hence the
principles and rules that license it would have to be innate.

As we have seen, there are a number of alternative theories that are much more surface"=oriented than
most variants of Transformational Grammar. These alternative theories often differ with
regard to particular assumptions that have been discussed in the preceding sections. For example, there are differences in the treatment of long"=distance dependencies that
have led to a proliferation of lexical items in Categorial Grammar (see Section~\ref{sec-pied-piping-cg}). As has been shown by \citet{Jacobs2008a}, \citet{Jackendoff2008a}
and others, approaches such as Categorial Grammar that assume that every phrase must have a functor/""head cannot explain certain constructions in a plausible way.
Inheritance"=based phrasal analyses that only list heads with a core meaning in the lexicon and have the constructions in which the heads occur determine the meaning
of a complex expression turn out to have difficulties with derivational morphology and with
accounting for alternative ways of argument realization (see Section~\ref{sec-val-morph},
\ref{inheritance-sec}, and~\ref{sec-mapping-between-levels}).
We therefore need a theory that handles argument structure changing processes in the lexicon and still has some kind of phrase structure or relevant schemata. Some variants
of GB/MP as well as LFG, HPSG, TAG and variants of CxG are examples of this kind of theory. Of these
theories, only HPSG and some variants of CxG make use of the same descriptive tools ((typed) feature
descriptions) for roots, stems, words, lexical rules and phrases. By using a uniform description for
all these objects, it is possible to formulate generalizations over the relevant objects. It is therefore
possible to capture what particular words have in common with lexical rules or phrases.
For example, the \bard\is{morphology} corresponds to a complex passive\is{passive} construction with
a modal verb. (\mex{1}) illustrates.
\eal
\ex 
\gll Das Rätsel ist lösbar.\\
     the puzzle is solvable\\
\ex 
\gll Das Rätsel kann gelöst werden.\\
     the puzzle can solved be\\
\glt `The puzzle can be solved.'
\zl
By using the same descriptive inventory for syntax and morphology, it is possible to capture
cross"=linguistic generalizations: something that is inflection/derivation in one language can be syntax in another.

It is possible to formulate principles that hold for both words and phrases and furthermore, it is possible to capture
cross"=linguistic generalizations or generalizations that hold for certain groups of languages. For example, languages can be divided
into those with fixed constituent order and those with more flexible or completely free constituent order. The corresponding types can be represented
with their constraints in a type hierarchy. Different languages can use a particular part of the hierarchy and also formulate
different constraints for each of the types (see \citealp[Section~9.2]{AW98a}).
HPSG\indexhpsg differs from theories such as LFG\indexlfg and TAG\indextag in that phrases are not
ontologically different from words. This means that there are no special c"=structures or tree structures. Descriptions of complex phrases simply have additional
features that say something about their daughters. In this way, it is possible to formulate cross"=linguistic generalizations
about dominance schemata. In LFG, the c"=structure rules are normally specified separately for each language.
% Allerdings
% lässt sich auch hier eine Konversion der Ansätze feststellen: \citet*{ADT2008a} zeigen, wie man
% Vererbungshierarchien für c"=Strukturannotationen verwenden kann.
Another advantage of consistent description is that one can capture similarities between words and
lexical rules, as well as between words and phrases. For example, a complementizer such as \emph{dass} `that' shares a number of
properties with a simple verb or with coordinated verbs in initial position:
\eal
\ex
\gll {}[dass] Maria die Platte kennt und liebt\\
	 {}\spacebr{}that Maria the record knows and loves\\
\glt `that Maria knows and loves the record'
\ex 
\gll {}[Kennt und liebt] Maria die Platte?\\
	 {}\spacebr{}knows and loves Maria the record\\
\glt `Does Maria know and love the record?'
\zl
The difference between the two linguistic objects mainly lies in the kind of phrase they select: the complementizer requires a sentence
with a visible finite verb, whereas the verb in initial position requires a sentence without a visible finite verb.

In Section~\ref{Abschnitt-Vererbung-HPSG}, a small part of an inheritance hierarchy was
presented. This part contains types
that probably play a role in the grammars of all natural languages: there are head"=argument combinations in every language. Without this
kind of combinatorial operation, it would not be possible to establish a relation between two concepts. The ability to create relations, however, is
one of the basic properties of language.

In addition to more general types, the type hierarchy of a particular language contains language"=specific types or those specific to a particular class
of languages. All languages presumably have one and two"=place predicates and for most languages (if not all), it makes sense to talk about verbs\is{verb}.
It is then possible to talk about one and two"=place verbs. Depending on the language, these can then be subdivided into intransitive and transitive.
Constraints are formulated for the various types that can either hold generally or be language"=specific.
In English\il{English}, verbs have to occur before their complements and therefore have the
\initialv $+$, whereas verbs in German have the \initialv $-$ and it is the lexical rule for initial
position that licenses a verb with an \initialv $+$.

The differing settings of the \initialv for German and English is reminiscent of
parameters\is{parameter}\is{acquisition|(} from GB"=Theory. There is one crucial difference,
however: it is not assumed that a language learner sets the \initialv for all
heads once and for all. The use of an \initialv is compatible with models of acquisition that assume that learners
learn individual words with their positional properties. It is certainly possible for the respective
words to exhibit different values for a particular feature. Generalizations about the position of
entire word classes are only learned at a later point in the acquisition process.\is{acquisition|)}

A hierarchy analogous to the one proposed by Croft (see Section~\ref{Abschnitt-Croft}) is given in Figure~\vref{Abbildung-Hierarchie}.
\begin{figure}
\centerfit{%
\begin{forest}
type hierarchy
[sign
  [stem
    [root
      [noun-root]
      [verb-root
        [intransitive-verb
          [strict-intransitive-verb
            [schlaf-\\sleep, instance]]]
        [transitive-verb
          [strict-transitive-verb
            [lieb-\\love, instance]]
          [ditransitive-verb
            [geb-\\give, instance]]]]]
    [complex-stem]]
  [word]
  [phrase
    [headed-phrase
      [head-argument-phrase]]]] 
\end{forest}
}
\caption{\label{Abbildung-Hierarchie}Section of an inheritance hierarchy with lexical entries and dominance schemata}
\end{figure}%
For inflected words, the relevant roots\is{root} are in the lexicon. Examples of this are \stem{schlaf} `sleep', \stem{lieb} `love' and \stem{geb}
`give'. In Figure~\ref{Abbildung-Hierarchie}, there are different subtypes of \type{root}, the
general type for roots: \eg \type{intrans-verb} for intransitive
verbs and \type{trans-verb} for transitive verbs. Transitive verbs can be further subdivided into strictly transitive verbs (those with nominative and accusative
arguments) and ditransitive verbs (those with nominative and both accusative and dative arguments). The hierarchy above would of course have to be refined considerably
as there are even further sub"=classes for both transitive and intransitive verbs. For example, one can divide intransitive verbs into unaccusative\is{verb!unaccusative}
and unergative\is{verb!unergative} verbs and even strictly transitive verbs would have to be divided into further sub"=classes (see \citealp[Section~2]{Welke2009a}).\pagebreak

%\addlines
In addition to a type for roots, the above figure contains types for stems\is{stem} and words\is{word}. Complex stems are complex objects that are
derived from simple roots but still have to be inflected (\stem{lesbar} `readable', \stem{besing}
`to sing about'). Words are objects that do not inflect. Examples of these
are the pronouns\is{pronoun} \emph{er} `he', \emph{sie} `she' etc.\ as well as prepositions. An inflected\is{inflection} form can be formed from a verbal stem
(\emph{geliebt} `loved', \emph{besingt} `sings about'). Relations between inflected words and (complex) stems can be formed again using derivation\is{derivation} rules. 
In this way, \emph{geliebt} `loved' can be recategorized as an adjective stem that must then be combined with adjectival endings (\emph{geliebt-e}).
The relevant descriptions of complex stems/words are subtypes of \type{complex-stem}
or \type{word}. These subtypes describe the form that complex words such as \emph{geliebte} must have. For a technical implementation of this, see
\citew[Section~3.2.7]{Mueller2002b}. Using dominance schemata, all words can be combined to form phrases. The hierarchy given here is of course by no means complete.
There are a number of additional valence classes and one could also assume more general types that simply describe one, two and three"=place predicates. 
Such types are probably plausible for the description of other languages. Here, we are only dealing with a small part of the type hierarchy in order to have
a comparison to the Croftian hierarchy: in Figure~\ref{Abbildung-Hierarchie}, there are no types for
sentence patterns with the form
[Sbj IntrVerb], but rather types for lexical objects with a particular valence
(V[\comps \sliste{ NP[\str] }]). Lexical rules can then be applied to the relevant lexical objects that license objects with another valence or introduce
information about inflection. Complete words can be combined in the syntax with relatively general rules, for example in head"=argument structures. The problems
from which purely phrasal approaches suffer are thereby avoided. Nevertheless generalizations about lexeme classes and the utterances that can be formed can be
captured in the hierarchy.

There are also principles in addition to inheritance hierarchies: the Semantics Principle\is{principle!Semantics} presented in Section~\ref{Abschnitt-HPSG-Semantik} holds
for all languages. The Case Principle\is{principle!Case} that we also saw is a constraint that only applies to a particular
class of languages, namely nominative"=accusative languages\is{case!nominative}\is{case!accusative}.
Other languages have an ergative"=absolutive system\is{case!ergative}\is{case!absolutive}.

The assumption of innate linguistic knowledge is not necessary for the theory of language sketched
here. As the discussion in Section~\ref{chap-innateness} has shown, the question of whether this kind of knowledge
exists has still not been answered conclusively. Should it turn out that this knowledge actually exists, the question arises of what exactly is innate. It would be a plausible assumption
that the part of the inheritance hierarchy that is relevant for all languages is innate together
with the relevant principles (\eg the constraints on Head-Argument structures and the Semantics Principle). It could, however, also be the case that only a part of the more generally
valid types and principles is innate since something being innate does not follow from the fact that
it is present in all languages (see also Section~\ref{Abschnitt-Universalien-Zusammenfassung}).\todostefan{Hier vielleicht noch etwas zur Variation sagen. Warum können benachbarte Dialekte nicht wild variieren?}

In sum, one can say that theories that describe linguistic objects using a consistent descriptive inventory and make use of inheritance hierarchies to capture
generalizations are the ones best suited to represent similarities between languages. Furthermore,
this kind of theory is compatible with both a positive and a negative
answer to the question of whether there is innate linguistic knowledge.

\section{How to develop linguistic theories that capture cross"=linguistic generalizations}
\label{sec-develop-theories-coregram}

In the previous section I argued for a uniform representation of linguistic knowledge at all
descriptive levels and for type hierarchies as a good tool for representing generalizations. This
section explores a way to develop grammars that are motivated by facts from several languages.

%\largerpage
If one looks at the current practice in various linguistic schools one finds two extreme ways of
approaching language. On the one hand, we have the Mainstream Generative Grammar (MGG) camp and, on the
other hand, we have the Construction Grammar/Cognitive Grammar camp. I hasten to say that what I state
here does not hold for all members of these groups, but for the extreme cases. The caricature of the
MGG scientist is that he is looking for underlying structures. Since these have to be the same for
all languages (poverty of the stimulus), it is sufficient to look at one language, say English. The
result of this research strategy is that one ends up with models that were suggested by the most
influential linguist for English and that others then try to find ways to accommodate other
languages. Since English has an NP VP structure, all languages have to have it. Since English reorders
constituents in passive sentences, passive is movement and all languages have to work this way. I
discussed the respective analyses of German in more detail in Section~\ref{sec-case-assignment}
and in Chapter~\ref{chap-scrambling-extraction-passive} and showed that the assumption that passive is movement
makes unwanted predictions for German, since the subject of passives stays in the object
position in German. Furthermore, this analysis requires the assumption of invisible expletives, that is,
entities that cannot be seen and do not have any meaning.

On the other extreme of the spectrum we find people working in Construction Grammar or without any
framework at all (see footnote~\ref{fn-ffs} on page~\ref{fn-ffs} for discussion) who claim that all languages are so different that we cannot even use the same
vocabulary to analyze them. Moreover, within languages, we have so many different objects that it is impossible (or too early) to state any
generalizations. Again, what I describe here are extreme positions and clichés.

In what follows, I sketch the procedure that we apply in the CoreGram project\footnote{%
\url{https://hpsg.hu-berlin.de/Projects/CoreGram.html}, \today.
} \citep{MuellerCoreGramBrief,MuellerCoreGram}. In the CoreGram project we work on a set of
typologically diverse languages in parallel:
\begin{itemize}
\item German\il{German}  \citep{MuellerLehrbuch1,MuellerPredication,MuellerCopula,MOe2011a,MOe2013a,MuellerArten,MuellerGS}
\item Danish  \citep{Oersnes2009a,MuellerPredication,MuellerCopula,MOe2011a,MOe2013a,MOe2013b,MOeDanish}
\item Persian\il{Persian} \citep*{MuellerPersian,MG2010a}
\item Maltese\il{Maltese} \citep{MuellerMalteseSketch}
\item Mandarin Chinese\il{Mandarin Chinese} \citep{Lipenkova2009a,ML2009a,ML2013a,MLChinese}
\item Yiddish\il{Yiddish} \citep{MOe2011a}
\item English\il{English} \citep{MuellerPredication,MuellerCopula,MOe2013a}
\item Hindi
\item Spanish\il{Spanish} \citep{Machicao-y-Priemer2015a}
\item French\il{French}
\end{itemize}

%\addlines[-1]
\noindent
These languages belong to diverse language families 
(Indo-European, % Germanic: German, Danish, Yiddish, English, Romance: Spanish, French,
                % Indo-Iranian: Hindi, Persian
                % Slavic: Czech
 Afro-Asiatic,  % Semitic: Maltese, Hebrew
 Sino-Tibetan) % Sinitic: Mandarin Chinese, 
and among the Indo-European languages the languages belong to different groups (Germanic, Romance,
Indo-Iranian). Figure~\ref{fig-lang-fams} provides an overview.
%
% moved above the figure
We work out fully formalized, computer"=processable grammar fragments in the framework of
HPSG\indexhpsg that have a semantics component. The details will not be discussed here, but the
interested reader is referred to \citew{MuellerCoreGram}. 
\begin{figure}
\centerfit{
\begin{forest}
[Languages
        [Indo-European
          [Germanic [Danish] [English] [German] [Yiddish] ]
          [Romance [French] [Spanish] ] 
          [Indo-Iranian [Hindi] [Persian] ] ]
        [Afro-Asiatic 
          [Semitic [Maltese] ] ]
        [Sino-Tibetan 
          [Sinitic [Mandarin Chinese] ] ] ]
\end{forest}
}
\caption{Language families and groups of the languages covered in the CoreGram project}\label{fig-lang-fams}
\end{figure}%

As was argued in previous sections, the assumption of innate language"=specific knowledge should be
kept to a minimum. This is also what Chomsky suggested in his Minimalist Program. There may even be no language"=specific innate knowledge at all, a view taken in Construction
Grammar/Cognitive Grammar. So, instead of imposing constraints from one language onto other languages, a bottom-up approach seems
to be more appropriate: grammars for individual languages should be motivated language"=internally. Grammars that share certain properties can be grouped in classes. This makes it possible
to capture generalizations about groups of languages and natural language as such. Let us consider a
few example languages: German, Dutch, Danish, English and French. If we start developing grammars for German and
Dutch, we find that they share a lot of properties: for instance, both are SOV and V2 languages and both have a
verbal complex. One main difference is the order of elements in the verbal complex. The situation
can be depicted as in Figure~\vref{fig-german-dutch}.
\begin{figure}
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
%    \tikzset{frontier/.style={distance from root=10\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set3) { ~Set 3~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ]  

    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch}; 

%    \node [left=of Set5] {V2};
     \node [left=of Set3] {\begin{tabular}{@{}c@{}}Arg St\\V2\\SOV\\VC\end{tabular}};
%    \node [right=of Set11] {SVO};

     \end{tikzpicture}
\caption{\label{fig-german-dutch}Shared properties of German and Dutch}
\end{figure}%
\largerpage[-1]
There are some properties that are shared between German and Dutch (Set 3). For instance, the
argument structure of lexical items, a list containing descriptions of syntactic and semantic properties of
arguments and the linking of these arguments to the meaning of the lexical items, is contained in Set 3. In
addition to the constraints for SOV languages, the verb position and the fronting of a
constituent in V2 clauses are contained in Set 3. The respective constraints are shared between the
two grammars. Although these sets are arranged in a hierarchy in Figure~\ref{fig-german-dutch} and
the following figures this has nothing to do with the type hierarchies that have been discussed in the previous subsection. These type
hierarchies are part of our linguistic theories and various parts of such hierarchies can be in different
sets: those parts of the type hierarchy that concern more general aspects can be in Set~3 in
Figure~\ref{fig-german-dutch} and those that are specific to Dutch or German are in the respective
other sets. When we add another language, say Danish, we get further differences. While German and Dutch are SOV, Danish
is an SVO language. Figure~\vref{fig-german-dutch-danish} shows the resulting situation: the
topmost node represents constraints that hold for all the languages considered so far (for instance the argument
structure constraints, linking and V2) and the node below it (Set~4) contains
constraints that hold for German and Dutch only.\footnote{%
  In principle, there could be constraints that hold for Dutch and Danish, but not for German or for
  German and Danish, but not for Dutch. These constraints would be removed from Set 1 and Set 2,
  respectively, and inserted into another constraint set higher up in the hierarchy. These sets are not
  illustrated in the figure and I keep the names Set~1 and Set~2 from Figure~\ref{fig-german-dutch} for the constraint sets for German
  and Dutch.
} For instance, Set~4 contains constraints regarding verbal complexes and SOV order.
% moved on top of the figure
The union of Set 4 and Set 5 is Set 3 of Figure~\ref{fig-german-dutch}.
\begin{figure}
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
    \tikzset{frontier/.style={distance from root=10\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set5) { ~Set 5~ };
               [.\node (Set4) { ~Set 4~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ] \node (Set6) { ~Set 6~ }; ] 

    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch};  \node [below=of Set6] {Danish};

    \node [left=of Set5] {\begin{tabular}{@{}c@{}}Arg Str\\V2\end{tabular}};
    \node [left=of Set4] {\begin{tabular}{@{}c@{}}SOV\\VC\end{tabular}};
%    \node [right=of Set11] {SVO};

     \end{tikzpicture}
\caption{\label{fig-german-dutch-danish}Shared properties of German, Dutch, and Danish}
\end{figure}%

If we add further languages, further constraint sets will be
distinguished. Figure~\vref{fig-german-dutch-danish-english-french} shows the situation that results
when we add English and French.
\begin{figure}
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
    \tikzset{frontier/.style={distance from root=15\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set8) { ~Set 8~ };
            [.\node (Set7) { ~Set 7~ };
               [.\node (Set4) { ~Set 4~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ] \node (Set6) { ~Set 6~ }; ] 
               [.\node (Set11) { ~Set 11~ }; \node (Set12) { ~Set 12~ };  \node (Set13) { ~Set 13~ }; ] 
    ]
    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch};  \node [below=of Set6] {Danish};
    \node [below=of Set12] {English}; \node [below=of Set13] {French}; 

    \node [left=of Set8] {Arg Str};
    \node [left=of Set7] {V2};
    \node [left=of Set4] {\begin{tabular}{@{}c@{}}SOV\\VC\end{tabular}};
    \node [right=of Set11] {SVO};

    \draw (Set11.south) -- (Set6.north);
    \end{tikzpicture}

\caption{\label{fig-german-dutch-danish-english-french}Languages and language classes}
\end{figure}%
Again, the picture is not complete since there are constraints that are shared by Danish and English
but not by French, but the general idea should be clear: by systematically working this way, we should
arrive at constraint sets that directly correspond to those that have been established in the typological
literature.

The interesting question is what will be the topmost set if we consider enough languages. At
first glance, one would expect that all languages have valence representations and linkings between
these and the semantics of lexical items (argument structure lists in the HPSG framework). However,
\citet{KM2012a} argue for an analysis of Oneida\il{Oneida} (a Northern Iroquoian language) that does not
include a representation of syntactic valence. If this analysis is correct, syntactic argument
structure would not be universal. It would, of course, be characteristic of a large number of
languages, but it would not be part of the topmost set. So this leaves us with just one candidate
for the topmost set from the area of syntax: the constraints that license the combination of two or more linguistic
objects. This is basically Chomsky's External Merge\is{Merge!External} without the binarity restriction\is{branching!binary}\footnote{%
  Note that binarity is more restrictive than flat structures: there is an additional constraint
  that there have to be exactly two daughters. As was argued in Section~\ref{Abschnitt-NPN-Konstruktion} one needs phrasal
  constructions with more than two constituents.
}. In addition, the topmost set would, of course, contain the basic machinery for representing phonology and semantics.  


It should be clear from what has been said so far that the goal of every scientist who works this
way is to find generalizations and to describe a new language in a way that reuses theoretical constructs
that have been found useful for a language that is already covered. However, as was explained above,
the resulting grammars should be motivated by data of the respective languages and not by facts from
other languages. In situations where more than one analysis would be compatible with a given dataset
for language X, the evidence from language Y with similar constructs is most welcome and can be used
as evidence in favor of one of the two analyses for language X. I call this approach the
\emph{bottom-up approach with cheating}: unless there is contradicting evidence, we can reuse
analyses that have been developed for other languages.  

Note that this approach is compatible with the rather agnostic view advocated by
\citet{Haspelmath2010a}, \citet{Dryer97a-u}, \citet[Section~1.4.2--1.4.3]{Croft2001a}, and others, who argue that descriptive categories should be
language-specific, that is, the notion of \emph{subject} for Tagalog is different from the one for English,
the category \emph{noun} in English is different from the category \emph{noun} in Persian and so on. Even if one
follows such extreme positions, one can still derive generalizations regarding constituent structure,
head-argument relations and so on. However, I believe that some categories can fruitfully be used
cross"=linguistically; if not universally, then at least for language classes. As \citet[\page
  692]{Newmeyer2010a} notes with regard to the notion of \emph{subject}: calling two items \emph{subject}
in one language does not entail that they have identical properties. The same is true for two
linguistic items from different languages: calling a Persian linguistic item \emph{subject} does not entail
that it has exactly the same properties as an English linguistic item that is called
\emph{subject}. The same is, of course, true for all other categories and relations, for instance, parts of speech:
Persian nouns do not share all properties with English nouns.\footnote{%
  Note that using labels like \emph{Persian Noun} and \emph{English Noun} (see for instance
  \citealp[Section~2]{Haspelmath2010a} for such a suggestion regarding case, \eg Russian Dative,
  Korean Dative, \ldots) is somehow strange since
  it implies that both Persian nouns and English nouns are somehow nouns. Instead of using the
  category \emph{Persian Noun} one could assign objects of the respective class to the class
  \emph{noun} and add a feature \textsc{language} with the value \type{persian}. This simple trick
  allows one to assign both objects of the type \emph{Persian Noun} and objects of the type
  \emph{English Noun} to the class \emph{noun} and still maintain the fact that there are
  differences. Of course, no theoretical linguist would introduce the \textsc{language} feature to
  differentiate between Persian and English nouns, but nouns in the respective languages have other features that
  make them differ. So the part of speech classification as noun is a generalization over nouns in
  various languages and the categories \emph{Persian Noun} and \emph{English Noun} are feature
  bundles that contain further, language"=specific information.

  %% \citet[\page 31]{Croft2001a} points out that it depends on the criteria that are chosen by the linguist
  %% whether languages like Makah have a Noun-Verb distinction or not. This is true, but as a result of
  %% the choice
}
\citet[\page 697]{Haspelmath2010b} writes: ``Generative linguists try to use as many crosslinguistic
  categories in the description of individual languages as possible, and this often leads to
  insurmountable problems.'' If the assumption of a category results in problems, they have to be
solved. If this is not possible with the given set of categories/features, new ones have to be
assumed. This is not a drawback of the methodology, quite the opposite is true: if we have found
something that does not integrate nicely into what we already have, this is a sign that we have discovered
something new and exciting. %For instance, 
If we stick to language-particular categories and features, it is much
harder to notice that a special phenomenon is involved, since all categories and features are
specific to one language anyway. Note also that not all speakers of a language community have
exactly the same categories. If one were to take the idea of language-particular category symbols to
an extreme, one would end up with person specific category symbols like \emph{Klaus-English-noun}.

%\addlines
\largerpage
After my talk at the MIT in 2013, members of the linguistics department objected to the
approach taken in the CoreGram project and claimed that it would not make any predictions as far as possible/impossible languages
are concerned. Regarding predictions two things must be said: firstly, predictions are being made on a
language particular basis. As an example consider the following sentences from \citet{Netter91}:


\eal
\ex 
\gll {}[Versucht, zu lesen], hat er das Buch nicht.\\
       \spacebr{}tried to read has he.\nom{} the.\acc{} book not\\
\glt `He did not try to read the book.'
\ex 
\gll {}[Versucht, einen Freund vorzustellen], hat er ihr noch nie.\\
       \spacebr{}tried a.\acc{} friend to.introduce has he.\nom{} her.\dat{} yet never\\
\glt `He never before tried to introduce a friend to her.'
\zl
When I first read these sentences I had no idea about their structure. I switched on my computer and typed them
in and within milliseconds I got an analysis of the sentences and by inspecting the result I realized
that these sentences are combinations of partial verb phrase fronting\is{partial verb phrase fronting} and the so-called third
construction\is{third construction} \citep[\page 439]{Mueller99a}. I had previously implemented analyses of both phenomena
but had never thought about the interaction of
the two. The grammar predicted that examples like (\mex{0}) are grammatical. Similarly the
constraints of the grammar can interact to rule out certain structures. So predictions about
ungrammaticality/impossible structures are in fact made as well.

Secondly, the topmost constraint set holds for all languages seen so far. It can be regarded as a
hypothesis about properties that are shared by all languages. This constraint set contains
constraints about the connection between syntax and information structure and such constraints allow
for V2 languages but rule out languages with the verb in penultimate position (see \citealp[\page
  50]{Kayne94a-u} for the claim that such languages do not exist. Kayne develops a complicated
syntactic system that predicts this). Of course, if a language is found that places the verb in penultimate
position for the encoding of sentence types or some other communicative effect, a more
general topmost set has to be defined. But this is parallel for Minimalist theories: if languages
are found that are incompatible with basic assumptions, the basic assumptions have to be revised. As
with the language particular constraints, the constraints in the topmost set make certain
predictions about what can be and what cannot be found in languages.

Frequently discussed examples such as those languages that form questions by reversing the order of
the words in a string (\citealp[\page 224]{Haider2015a-u}; \citealp{MMGRRBW2003a}) need not be ruled out by the grammar, since they are ruled out by
language external constraints: we simply lack the working memory to do such complex
computations\is{performance}.
% Haider Mail 02.07.18 [1 [2 [3 [4 5]]]]  --> .... [5 [4 --]]...  --> .... [[5 [4 --]] [3  --]]...  usw.


A variant of this argument comes from David Pesetsky\ia{Pesetsky, David} and was raised in Facebook discussions of an
article by Paul Ibbotson\ia{Ibbotson, Paul} and Michael Tomasello\ia{Tomasello, Michael}
published in The Guardian\footnote{%
\emph{The roots of language: What makes us different from other animals?} Published
2015-11-05.\newline \url{http://www.theguardian.com/science/head-quarters/2015/nov/05/roots-language-what-makes-us-different-animals}, 2018/04/25.
}. 
%% how [Tomasello's] `prefabricated phrases' proposal could scale up so these kids end up speaking a verb-second language (when and how do they learn a structure-dependent rule that moves a particular verb and something else to designated positions) — and how he [Tomasello] would explain the frequent appearance of verb-second vs. the non-existence of obligatory verb-third crosslinguistically.
Pesetsky claimed that Tomasello's theory of language
acquisition could not explain why we find V2 languages but no V3 languages. First, I do not know of
anything that blocks V3 languages in current Minimalist theories. So per se the fact that V3
languages may not exist cannot be used to support any of the competing approaches. Of course, the
question could be asked whether the V3 pattern would be useful for reaching our communicative goals
and whether it can be easily acquired. Now, with V2 as a pattern it is clear that we have exactly
one position that can be used for special purposes in the V2 sentence (topic or focus). For monovalent and bivalent verbs we
have an argument that can be placed in initial position. The situation is different for the
hypothetical V3 languages, though: If we have monovalent verbs like \emph{sleep}, there is nothing
for the second position. As Pesetsky pointed out in the answer to my comment on a blog post, languages solve such
problems by using expletives. For instance some languages insert an expletive to mark subject
extraction in embedded interrogative sentences, since otherwise the fact that the subject is
extracted would not be recognizable by the hearer. So the expletive helps to make the structure
transparent. V2 languages also use expletives to fill the initial position if speakers want to avoid
something in the special, designated position:
\ea
\gll Es kamen drei Männer zum Tor hinein.\\
     \expl{} came three man to.the gate in\\
\glt `Three man came through the gate.'
\z
In order to do the same in V3 languages one would have to put two expletives in front of the
verb. So there seem to be many disadvantages of a V3 system that V2 systems do not have and hence
one would expect that V3 systems are less likely to come into existence. If they existed, they would
be expected to be subject to change in the course of time; \eg omission of the expletive with
intransitives, optional V2 with transitives and finally V2 in general. With the new modeling
techniques for language acquisition and agent"=based community simulation one can actually simulate
such processes and I guess in the years to come, we will see exciting work in this area. 

\citew[\page 106]{Cinque99a-u} suggested a cascade of functional projections to account for
reoccurring orderings in the languages of the world. He assumes elaborate tree structures to play a
role in the analysis of all sentences in all languages even if there is no evidence for respective
morphosyntactic distinctions in a particular language (see also \citealp[\page 55]{CR2010a}). In the
latter case, Cinque assumes that the respective tree nodes are empty. Cinque's results could be
incorporated in the model advocated here. We would define part of speech categories and
morpho-syntactic features in the topmost set and state linearization constraints that enforce the
order that Cinque encoded directly in his tree structure. In languages in which such categories are
not manifested by lexical material, the constraints would never apply. Neither empty elements nor
elaborate tree structures would be needed. Thus Cinque's data could be
covered in a better way in an HPSG with a rich UG but I, nevertheless, refrain from introducing 400 categories (or
features) into the theories of all languages and, again, I point out that such a rich and
language-specific UG is implausible from a genetic\is{gene} point of view. Therefore, I wait for other, probably functional, explanations of the Cinque data.

Note also that implicational universals\is{universal!implicative} can be derived from hierarchically organized constraint sets
as the ones proposed here. For instance, one
can derive from Figure~\ref{fig-german-dutch-danish-english-french} the implicational statement that
all SVO languages are V2 languages, since there is no language that has constraints from Set~4 that
does not also have the constraints of Set~7. Of course, this implicational statement is wrong, since there are
lots and lots of SOV languages and just exceptionally few V2 languages. So, as soon as we add other
languages as for instance Persian or Japanese, the picture will change.

The methodology suggested here differs from what is done in MGG, since MGG stipulates the general
constraints that are supposed to hold for all languages on the basis of general specualtions about
language. In the best case, these general assumptions are fed by a lot of experience with different
languages and grammars, in the worst case they are derived from insights gathered from one or more
Indo"=European languages. Quite often impressionistic data is used to motivate rather far"=reaching
fundamental design decisions \citep{Fanselow2009a,SR2012a,Haider2016a}. It is interesting to note
that this is exactly what members of the MGG camp reproach typologists for. \citet{EL2009a} pointed out
that counterexamples can be found for many alleged universals. A frequent response to this is that
unanalyzed data cannot refute grammatical hypotheses (see, for instance, \citealp[\page
  454]{Freidin2009a}). In the very same way it has to be said that unanalyzed data should not be
used to build theories on \citep{Fanselow2009a}. In the CoreGram project, we aim to develop
broad"=coverage grammars of several languages, so those constraints that make it to the top node are
motivated and not stipulated on the basis of intuitive implicit knowledge about language.

Since it is data"=oriented and does not presuppose innate language"=specific knowledge, this research strategy is compatible with work carried out in Construction Grammar (see
\citealp[\page 481]{Goldberg2013b} for an explicit statement to this end) and in any case it should also be compatible with the Minimalist
world.








%      <!-- Local IspellDict: en_US-w_accents -->
