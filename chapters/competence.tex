%% -*- coding:utf-8 -*-

\chapter{The competence/performance distinction}
\label{Abschnitt-Diskussion-Performanz}\label{chap-competence-performance}

\begin{sloppypar}
The distinction\nocite{VL2006a} between competence and performance\is{competence|(}\is{performance|(}
\citep[Section~1.1]{Chomsky65a}, which is assumed by several theories of grammar, was already
discussed in Section~\ref{Abschnitt-Kompetenz-Performanz-TAG} about the analysis of scrambling and
verbal complexes in TAG. Theories of competence are
intended to describe linguistic knowledge and performance theories are assigned the task of
explaining how linguistic knowledge is used as well as why mistakes are made in speech production
and comprehension. A classic example in the competence/""performance discussion are cases of
center self"=embedding\is{self"=embedding}. \citet[\page 286]{CM63a} discuss the following example with
recursively\is{recursion} embedded relative clauses\is{relative clause}: 
\end{sloppypar}
\ea
(the rat (the cat (the dog chased) killed) ate the malt)
\z
(\mex{1}b) is a corresponding example in German:
\eal
\ex 
\gll dass der Hund bellt, der die Katze jagt, die die Maus kennt, die im Keller lebt\\
     that the dog.\mas{} barks that.\mas{} the cat chases that.\fem{} the mouse knows who in.the basement lives\\
\glt `that the dog that chases the cat that knows the mouse who is living in the basement is barking'
\ex\label{Bsp-Selbsteinbettung} 
\gll dass er Hund, [$_1$ der die Katze, [$_2$ die die Maus, [$_3$ die im Keller lebt,  $_3$] kennt, $_2$] jagt $_1$] bellt\\
     that the dog {} that the cat {} that the mouse      {}    who in.the basement lives {} knows {} chases {} barks\\
\zl
%
\largerpage
The examples in (\mex{-1}) and (\mex{0}b) are entirely incomprehensible for most people.
If one rearranges the material somewhat, it is possible to process the sentences and assign a
meaning to them.\footnote{%
  The sentence in (\mex{0}a) can be continued following the pattern that was used to create the
  sentence. For instance by adding \emph{die unter der
    Treppe lebte, die meine Freunde repariert haben} `who lived under the staircase which my
  friends repaired'. This shows that a restriction of the number of elements that depend on one head
to seven  \citep[\page 322]{Leiss2003a} does not restrict the set of the sentences that are
generated or licensed by a grammar to be finite. There are at most two dependents of each head in
(\mex{0}a). The extraposition of the relative clauses allows the hearer to group material into
processable and reducible chunks, which reduces the cognitive burden during processing.

  This means that the restriction to seven dependents does not cause a finitization of recursion
  (``Ver\-end\-lichung von Rekursivität\is{recursion}'') as was claimed by \citet[\page 322]{Leiss2003a}.
  Leiss argued that Miller could not use his insights regarding short term memory, since he worked
  within Transformational Grammar\is{Transformational Grammar} rather than in \dg. The discussion
  shows that dependency plays an important role, but that linear order is also important for processing.
}
For sentences such as (\mex{0}b), it is often assumed that they fall within our grammatical competence, that is, we possess
the knowledge required to assign a structure to the sentence, although the processing of utterances such as (\mex{0}b) exceeds
language"=independent abilities of our brain.
In order to successfully process (\mex{0}b), we would have to retain the first five noun phrases and corresponding hypotheses
about the further progression of the sentence in our heads and could only begin to combine syntactic material when the verbs appear.
Our brains become overwhelmed by this task. These problems do not arise when analyzing (\mex{0}a) as it is possible
to immediately begin to integrate the noun phrases into a larger unit.

Nevertheless, center self"=embedding of relative clauses can also be constructed in such a way that our brains can handle them. Hans
Uszkoreit\ia{Uszkoreit, Hans} (p.\,c.\ 2009)
% ??? lecture notes?
gives the following example:
\ea
\label{Bsp-Selbsteinbettung-Uszkoreit}
\gll Die Bänke, [$_1$ auf denen damals die Alten des Dorfes, [$_2$ die allen Kindern, [$_3$ die vorbeikamen $_3$], freundliche Blicke zuwarfen $_2$], 
lange Stunden schweigend nebeneinander saßen $_1$], mussten im letzten Jahr einem~~~~~~~~ Parkplatz weichen.\\
the benches {} on which back.then the old.people of.the village {} that all children {} that came.by {} friendly glances gave {}
long hours silent next.to.each.other sat {} must in.the last year a car.park give.way.to\\
\glt `The benches on which the older residents of the village, who used to give friendly glances to all the children who came by, used to sit silently next to one 
another for hours had to give way to a car park last year.'
\z
Therefore, one does not wish to include in the description of our grammatical knowledge that
relative clauses are not allowed to be included inside each other as in (\ref{Bsp-Selbsteinbettung})  
as this would also rule out (\ref{Bsp-Selbsteinbettung-Uszkoreit}).

\addlines[-1]
We can easily accept the fact that our brains are not able to process structures past a certain degree of complexity and also that corresponding utterances then become unacceptable.
The contrast in the following examples is far more fascinating:\footnote{%
See \citew[\page 227]{GT99a}. \citet[\page 178]{Frazier85a-u} 
attributes the discovery of this kind of sentences to Janet Fodor\ia{Fodor, Janet Dean}.
}
\eal
\ex[\#]{
The patient [ who the nurse [ who the clinic had hired ] admitted ] met Jack.
}
\ex[*]{
The patient who the nurse who the clinic had hired met Jack.
}
\zl
Although (\mex{0}a)  is syntactically well"=formed and (\mex{0}b) is not, \citet{GT99a} were able to
show that (\mex{0}b) is rated better by speakers than (\mex{0}a). It does not occur to some people that an entire
VP is missing\is{Missing VP effect}. There are a number of explanations for this fact, all of which in
some way make the claim that previously heard words are forgotten as soon as new words are heard and
a particular degree of complexity is exceeded (\citealp[\page 178]{Frazier85a-u}; \citealp{GT99a}). 

Instead of developing grammatical theories that treat (\ref{Bsp-Selbsteinbettung}) and (\mex{0}a) as
unacceptable and (\ref{Bsp-Selbsteinbettung-Uszkoreit}) and (\mex{0}b) as acceptable, descriptions
have been developed that equally allow (\ref{Bsp-Selbsteinbettung}),
(\ref{Bsp-Selbsteinbettung-Uszkoreit}), and (\mex{0}a) (competence models) and then additionally
investigate the way utterances are processed in order to find out what kinds of structures our
brains can handle and what kinds of structures it cannot.
The result of this research is then a performance model (see \citew{Gibson98a}, for example).
This does not rule out that there are language"=specific differences affecting language processing.
For example, \citet*{VSLK2010a} have shown that the effects that arise in center self"=embedding structures in German are different from those
that arise in the corresponding English cases such as (\mex{0}):
due to the frequent occurrence of verb"=final structures in German, speakers of German were able to better store predictions about the
anticipated verbs into their working memory (p.\,558).

Theories in the framework of Categorial Grammar\is{Categorial Grammar (CG)},
GB\indexgb, \lfg, \gpsg and \hpsg are theories about our linguistic competence.\footnote{%
  For an approach where the parser is equated with UG, see \citew[Section~3.4]{AC86a}.
  For a performance"=oriented variant of Minimalism, see \citew{Phillips2003a}.

  In Construction Grammar\indexcxg, the question of whether a distinction between competence and
  performance would be justified at all is controversially discussed (see Section~\ref{sec-performance-cxg}).
  \citet*{FSCK99a} also suggest a model -- albeit for different reasons -- where grammatical properties considerably affect
  processing properties. The aforementioned authors work in the framework of Optimality Theory\indexot and show that the OT constraints that
  they assume can explain parsing preferences. OT is not a grammatical theory on its own but rather a meta theory.
  It is assumed that there is a component GEN that creates a set of candidates. A further component EVAL then chooses the most optimal candidate from this set
  of candidates. GEN contains a generative grammar of the kind that we have seen in this book. Normally, a GP/MP variant\indexgb or also LFG\indexlfg is assumed
  as the base grammar. If one assumes a transformational theory, then one automatically has a
  problem with the Derivational Theory of Complexity\is{Derivational Theory of Complexity (DTC)} that
  we will encounter in the following section. If one wishes to develop OT parsing models, then one has to make reference to representational variants of GB
  as the aforementioned authors seem to.%
}
If we want to develop a grammatical theory that directly reflects our cognitive abilities, then there should also be a corresponding performance model to go with
a particular competence model. In the following two sections, I will recount some arguments from \citet{SW2011a} in favor of constraint"=based\is{constraint"=based grammar} theories such as
GPSG, LFG and HPSG.

\section{The derivational theory of complexity}
\label{sec-dtc}

%\largerpage[-1]
The first point discussed by \citet{SW2011a} is the Derivational Theory of Complexity.
In\is{Derivational Theory of Complexity (DTC)|(}\is{transformation|(}  the early days of Transformational Grammar,
it was assumed that transformations were cognitively real, that is, it is possible to measure the consumption of resources
that transformations have.
A sentence that requires more transformations than the analysis of another sentence should therefore also be more difficult for humans
to process. The corresponding theory was dubbed the \emph{Derivational Theory of Complexity} (DTC) and initial experiments
seemed to confirm it \citep{MMK64a,SP65a,CO66a}, so that in 1968 Chomsky still assumed that the
\pagebreak
Derivational Theory of Complexity was in fact correct
\citep[\page 249--250]{Chomsky76b-u}.\footnote{%
In the Transformational Grammar literature, transformations were later viewed as a metaphor
(\citealp[\page 170]{Lohnstein2014a}, also in \citealp[Footnote~4]{Chomsky2001a-u}), that is,
it was no longer assumed to have psycholinguistic reality. In \emph{Derivation by phase} and
\emph{On phases}, Chomsky refers once again to processing aspects such as computational and memory load\is{memory} (Chomsky \citeyear[\page 11, 12,
   15]{Chomsky2001a-u}; \citeyear[\page 3, 12]{Chomsky2007a}; \citeyear[\page 138, 145, 146,
   155]{Chomsky2008a}). See also \citew[\page 440]{Marantz2005a} and
\citew{Richards2015a}. Trinh (\citeyear[\page 17]{Trinh2011a}; \citeyear[\page 9]{Trinh2019a}) cites Chomsky (p.c.) with the following quote:
``As speaking involves cognitive effort, Pronunciation Economy might be derived from the general principle of minimizing computation.'' 

  A structure building operation that begins with words and is followed by transformations/internal
  merge and further combinations, as recently assumed by theories in the Minimalist Program,
  is psycholinguistically implausible for sentence parsing. See \citew{Labelle2007a} and Section~\ref{Abschnitt-Inkrementelle-Verarbeitung} for
  more on incremental processing.

  \citet[\page 6]{Chomsky2007a} (written later than \emph{On phases}) seems to adopt a constraint"=based\is{constraint"=based grammar} view. He writes that ``a Merge-based system involves parallel operations''
  and compares the analysis of an utterance with a proof and explicitly mentions the competence/performance distinction.
 %
} 
Some years later, however, most psycholinguists rejected the DTC. For discussion of several experiments that testify against
the DTC, see \citew*[\page 320--328]{FBG74a-u}. One set of phenomena where the DTC makes incorrect predictions for respective analyses is that of elliptical\is{ellipsis} constructions, for example
\citep*[\page 324]{FBG74a-u}: in elliptical constructions, particular parts of the utterance are left out or replaced by auxiliaries.
In transformation"=based approaches, it was assumed that (\mex{1}b) is derived from (\mex{1}a) by means of deletion\is{deletion} of
\emph{swims} and (\mex{1}c) is derived from (\mex{1}b) by inserting \emph{do}.\footnote{%
  Similar analyses are assumed today in the Minimalist Program. For example, \citet[\page
    63]{Trinh2011a} assumes that VP ellipsis is deletion at Phonological Form (PF). This means that
% zitiert Lasnik99a 
  a complete structure is built which is then not pronounced. Since he talks about cognitive efforts
  and computation with respect to the activity of speaking (p.\,17), it follows that he regards the
  structures he is assuming as congnitively real.
}
\eal
\ex John swims faster than Bob swims.
\ex John swims faster than Bob.
\ex John swims faster than Bob does.
\zl
The DTC predicts that (\mex{0}b) should require more time to process than (\mex{0}a),
since the analysis of (\mex{0}b) first requires to build up the structure in (\mex{0}a) and then delete \emph{swims}. This prediction was not confirmed.

Similarly, no difference could be identified for the pairs in (\mex{1}) and (\mex{2}) even though one of the sentences, given the relevant theoretical
assumptions, requires more
transformations for the derivation from a base structure \citep*[\page 324]{FBG74a-u}.
\eal
\ex John phoned up the girl.
\ex John phoned the girl up.
\zl
\eal
\ex The bus driver was nervous after the wreck.
\ex The bus driver was fired after the wreck.
\zl
In (\mex{-1}), we are dealing with local reordering of the particle\is{verb!particle} and the object. (\mex{0}b) contains a passive clause that
should be derived from an active clause\is{passive} under Transformational Grammar assumptions. If we compare this sentence with an equally long sentence
with an adjective, like (\mex{0}a), the passive clause should be more difficult to process. This is, however, not the case.

It is necessary to add two qualifications to Sag~\& Wasow's claims: if one has experimental data that show that the DTC makes incorrect predictions for a particular
analysis, this does not necessarily mean that the DTC has been disproved. One could also try to find a different analysis for the phenomenon in question.
For example, instead of a transformation that deletes material, one could assume empty elements for the analysis of elliptical structures that are inserted
directly into the structure without deleting any material (see page~\pageref{np-epsilon} for the
assumption of an empty nominal head in structures with noun ellipsis in German). Data such as (\mex{-2}) would then be irrelevant to the discussion.\footnote{%
  \citet[Chapters~1 and~7]{CJ2005a} argue in favor of analyzing ellipsis as a semantic or pragmatic phenomenon rather than a syntactic
  one anyway.
} 
However, reordering such as (\mex{-1}b) and the passive in (\mex{0}b) are the kinds of phenomena that are typically explained using transformations.

\largerpage
The second qualification pertains to analyses for which there is a representational variant: it is often said that transformations are simply
metaphors (Jackendoff \citeyear[\page
22--23]{Jackendoff2000a}; \citeyear[\page 5, 20]{Jackendoff2007a}): for example, we have seen that extractions with a transformational grammar yield structures that
are similar to those assumed in \hpsg. Figure~\vref{Abbildung-zyklisch-Perkolation} shows cyclic\is{cycle!transformational} movement in \gbt compared
to the corresponding \hpsg analysis.
\begin{figure}
\hfill%
\adjustbox{valign=c}{%
\begin{forest}
[CP
	[NP
		[\_$_ i$]]
	[\hspaceThis{$'$}C$'$
		[C]
		[VP
			[NP]
			[\hspaceThis{$'$}V$'$
				[V]
				[NP
					[\_$_ i$]]]]]]
\end{forest}}
\hfill
\adjustbox{valign=c}{%
\begin{forest}
[CP/NP
	[C]
	[VP/NP
		[NP]
		[V$'$/NP
			[V]
			[NP/NP
				[\_$_ i$]]]]]
\end{forest}}
\hfill\mbox{}%
\caption{Cyclic movement vs.\ feature percolation}\label{Abbildung-zyklisch-Perkolation}
\end{figure}%

In GB, an element is moved to the specifier position of CP (SpecCP) and can then be moved from there to the next higher SpecCP position.
\eal\settowidth\jamwidth{(HPSG)}
\ex
Chris$_i$, we think [\sub{CP} \_$_i$ Anna claims [\sub{CP} \_$_i$ that David saw \_$_i$]].\jambox{(GB)}
\ex
Chris$_i$, we think [\sub{CP/NP} Anna claims [\sub{CP/NP} that David saw \_$_i$]].\jambox{(HPSG)}
\zl
In HPSG, the same effect is achieved by structure sharing\is{structure sharing}. Information about a long"=distance dependency
is not located in the specifier node but rather in the mother node of the projection itself. In Section~\ref{Abschnitt-Eleminierung-leerer-Elemente},
I will discuss various ways of eliminating empty elements from grammars. If we apply these techniques to structures such as the GB structure
in Figure~\ref{Abbildung-zyklisch-Perkolation}, then we arrive at structures where information about missing elements is integrated into the
mother node (CP) and the position in SpecCP is unfilled. This roughly corresponds to the HPSG structure in Figure~\ref{Abbildung-zyklisch-Perkolation}.\footnote{%
In Figure~\ref{Abbildung-zyklisch-Perkolation}, additionally the unary branching of C$'$ to CP was omitted in the tree on the right so that C combines directly with VP/NP
to form CP/NP.%
}
It follows from this that there are classes of phenomena that
% \todoandrew{changed `for' to
  % `about'. OK?} 
can be spoken about in terms of transformations without expecting empirical differences with regard to
performance when compared to transformation"=less approaches.
However, it is important to note that we are dealing with an S"=structure in the left"=hand tree in Figure~\ref{Abbildung-zyklisch-Perkolation}. As soon as one assumes
that this is derived by moving constituents out of other structures, this equivalence of approaches disappears.
\is{transformation|)}\is{Derivational Theory of Complexity (DTC)|)}

\section{Incremental processing}
\label{Abschnitt-Inkrementelle-Verarbeitung}

The next important point mentioned by \citet{SW2011a} is the fact that both comprehension and production of language take places incrementally.
As soon as we hear or read even the beginning of a word, we begin to assign meaning and to create structure.
In the same way, we sometimes start talking before we have finished planning the entire utterance.
This is shown by interruptions and self"=correction in spontaneous speech \citep{CW98a,CFT2002a}.
When it comes to processing spoken speech, \citet{TSKES96a} have shown that we access a word as soon
as we have heard a part of it (see also \citealp{Marslen-Wilson75a}).  The authors of the study carried out an experiment where participants were instructed
to pick up particular objects on a grid and reorganize them. Using eye"=tracking measurements,
Tanenhaus and colleagues could then show that the participants could identify the object in question
earlier if the sound sequence at the beginning of the word was unambiguous than in cases 
where the initial sounds occurred in multiple words. An example for this is a configuration with a candle and candy: \emph{candy} and \emph{candle}
both begin with \emph{can} such that speakers could not yet decide upon hearing this sequence which lexical entry should be accessed. Therefore, there
was a slight delay in accessing the lexical entry when compared to words where the objects in question did not contain the same segment at the
start of the word \citep[\page 1633]{TSKES95a}.

\addlines
If complex noun phrases were used in the instructions (\emph{Touch the starred yellow
  square}), the participants' gaze fell on the object in question 250ms after it was unambiguously identifiable.
  This means that if there was only a single object with stars on it, then they looked at it after they heard
  \emph{starred}. In cases where there were starred yellow blocks as well as squares, they looked at the square
  only after they had processed the word \emph{square} \citep[\page 1632]{TSKES95a}.
The planning and execution of a gaze lasts 200ms. From this, one can conclude that hearers combine words directly
and as soon as enough information is available, they create sufficient structure in order to capture
the (potential) meaning of an expression and react accordingly.
This finding is incompatible with models that assume that one must have heard a complete noun phrase or even a complete utterance
of even more complexity before it is possible to conclude anything about the meaning of a phrase/utterance. 
In particular, analyses in the Minimalist Program\indexmp which assume that only entire phrases or so"=called phases\is{phase}\footnote{%
     Usually, only CP and vP are assumed to be phases.
}
are interpreted (see \citealp{Chomsky99a} and also \citealp[\page 441]{Marantz2005a}, who explicitly contrasts the
MP\indexmp to Categorial Grammar\indexcg) must therefore be rejected as inadequate from a psycholinguistic perspective.\footnote{%
% 74
\citet[\page 729--730]{Sternefeld2006a-u} points out that in theories in the Minimalist Program, the common assumption of uninterpretable
features is entirely unjustified. Chomsky assumes that there are features that have to be deleted in the course of a derivation
since they are only relevant for syntax. If they are not checked, the derivation crashes at the interface to semantics.
It follows from this that NPs should not be interpretable under the assumptions of these theories since they contain a number of features
that are irrelevant for the semantics and have to therefore be deleted (see
Section~\ref{sec-features-minimalism} of this book and \citealp{Richards2015a}).
As we have seen, these kinds of theories are incompatible with the facts.
}$^,$\footnote{%
  It is sometimes claimed that current Minimalist theories are better suited to explain production (generation)
  than perception (parsing). But these models are as implausible for generation as they are for parsing. The
  reason is that it is assumed that there is a syntax component that generates structures that are
  then shipped to the interfaces. This is not what happens in generation though. Usually speakers
  know what they want to say (at least partly), that is, they start with semantics.
}

With contrastive\is{contrast} emphasis\is{prosody} of individual adjectives in complex noun phrases
(\eg \emph{the BIG blue triangle}), hearers assumed that there must be a corresponding counterpart to the reference object, \eg a small blue triangle.
The eye"=tracking studies carried out by \citet{TSKES96a} have shown that taking this kind of information into account
results in objects being identified more quickly.

Similarly, \citet{ATAF2004a} have shown, also using eye"=tracking studies, that hearers tend to direct their gaze
to previously unmentioned objects if the interlocutor interrupts their speech with \emph{um} or \emph{uh}.
This can be traced back to the assumption that hearers assume that describing previously unmentioned objects is more
complex than referring to objects already under discussion. The speaker can create more time for himself
by using \emph{um}
or \emph{uh}.

%\largerpage
Examples such as those above constitute evidence for approaches that assume that when processing language, information from all available
channels is used and that this information is also used as soon as it is available and not only after the structure of the entire utterance or
complete phrase has been constructed.
The results of experimental research therefore show that the hypothesis of a strictly modular\is{modularity} organization
of linguistic knowledge must be rejected.
Proponents of this hypothesis assume that the output of one module constitutes the input of another without a given module having access
to the inner states of another module or the processes taking place inside it.
For example, the morphology module could provide the input for syntax and then this would be
processed later by the semantic module. One kind of evidence for this kind of organization of linguistic knowledge that is often cited
are so"=called \emph{garden path sentences} such as (\mex{1}):
\eal
\ex\label{bsp-horse-past-barn} 
The horse raced past the barn fell.
\ex The boat floated down the river sank.
\zl
The vast majority of English speakers struggle to process these sentences since their parser is led down a garden path
as it builds up a complete structure for (\mex{1}a) or (\mex{1}b) only then to realize that there is another
verb that cannot be integrated into this structure.
\eal
\ex The horse raced past the barn.
\ex The boat floated down the river.
\zl
However, the actual structure of (\mex{-1}) contains a reduced relative clause (\emph{raced past
  the barn} or \emph{floated down the river}). That is the sentences in (\ref{bsp-horse-past-barn})
are semantically equivalent to the sentences in (\mex{1}):
\eal
\ex The horse that was raced past the barn fell.
\ex The boat that was floated down the river sank.
\zl
The failure of the parser in these cases was explained by assuming that syntactic processing
such as constructing a sentence from NP and VP take place independently of the processing of other constraints.
As \citet{CS85a} and others have shown, yet there are data that make this explanation seem less plausible: 
if (\ref{bsp-horse-past-barn}) is uttered in a relevant context, the parser is not misled.
In (\mex{1}), there are multiple horses under discussion and each NP is clearly identified by a relative
clause. The hearer is therefore prepared for a relative clause and can process the reduced relative clause
without being led down the garden path, so to speak.
\ea
The horse that they raced around the track held up fine. The horse that was raced down
    the road faltered a bit. And the horse raced past the barn fell.
\z

\noindent
By exchanging lexical material, it is also possible to modify (\ref{bsp-horse-past-barn}) in such way as to
ensure that processing is unproblematic without having to add additional context. It is necessary
to choose the material so that the interpretation of the noun as the subject of verb in the reduced
relative clause is ruled out. Accordingly, \emph{evidence} in (\mex{1}) refers to an inanimate noun.
It is therefore not a possible agent of \emph{examined}. A hypothesis with \emph{evidence} as the
agent of \emph{examined} is therefore never created when processing this sentence
\citep{SW2011a}.\todostefan{Trueswell, Ferreira and Clifton zitieren \citet{TTG94a,FC86a}}
\ea
The evidence examined by the judge turned out to be unreliable.
\z
\largerpage[2]
Since processing proceeds incrementally, it is sometimes assumed that realistic grammars should be obliged to immediately assign a constituent structure to previously heard
material \citep{AS82a,Steedman89b-u,Hausser92a-u}.
Proponents of this view would assume a structure for the following sentence where every word forms a constituent with the
preceding material:

\ea
\gll {}[[[[[[[[[[[[[[Das britische] Finanzministerium] stellt] dem] angeschlagenen] Bankensystem] des] Landes] mindestens] 200] Milliarden] Pfund] zur]~~~~~~~~~ Verfügung].\\
{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}\spacebr{}the British treasury provides the
crippled banking.system of.the country at.least 200 billion pounds to use\\
\glt 'The British Treasury is making at least 200 billion pounds available to the crippled banking system.'
\z
\citet{Pulman85a}, \citet{Stabler91a} and \citet[\page 301--308]{SJ93a} have shown, however, that it is possible to build semantic structures incrementally,
using the kind of phrase structure grammars we encountered in Chapter~\ref{Kapitel-PSG}. This means that a partial semantic representation for the
string \emph{das britische} `the British' can be computed without having to assume that the two words form a constituent in (\mex{0}).
Therefore, one does not necessarily need a grammar that licenses the immediate combination of words directly.
Furthermore, \citew{SJ93a} point out that from a purely technical point of view, synchronous processing is more costly than asynchronous processing since
synchronous processing requires additional mechanisms for synchronization whereas asynchronous processing processes information as soon as it
becomes available (p.\,297--298). Shieber and Johnson do not clarify whether this also applies to synchronous/""asynchronous processing of syntactic and semantic
information. See \citew{SJ93a} for incremental processing and for a comparison of Steedman's Categorial Grammar\indexcg and TAG\indextag.

What kind of conclusions can we draw from the data we have previously discussed? Are there further data that can help to determine the kinds
of properties a theory of grammar should have in order to count as psycholinguistically plausible? \citet*{SWB2003a} and \citet{SW2011a,SW2015a}
list the following properties that a performance"=compatible competence grammar should have:\footnote{%
  Also, see \citew{Jackendoff2007a} for reflections on a performance model for a constraint"=based\is{constraint"=based grammar}, surface"=oriented linguistic
  theory.
}
\begin{itemize}
\item surface"=oriented
\item model"=theoretic and therefore constraint"=based\is{constraint"=based grammar}
\item sign-based organization
\item strictly lexicalist
\item representational underspecification of semantic information
\end{itemize}

\largerpage[2]
\noindent
Approaches such as CG\indexcg, GPSG\indexgpsg, LFG\indexlfg, HPSG\indexhpsg, CxG\indexcxg and TAG\indextag are surface"=oriented
since they do not assume a base structure from which other structures are derived via transformations. Transformational\is{transformation}
approaches, however, require additional assumptions.\footnote{%
	An exception among transformational approaches is \citew{Phillips2003a}. Phillips assumes that structures relevant for phenomena such as ellipsis\is{ellipsis},
	coordination\is{coordination} and fronting
	are built up incrementally. These constituents are then reordered in later steps by transformations. For example, in the analysis of (i), the string
	\emph{Wallace saw Gromit in} forms a constituent where \emph{in} is dominated by a node with the label P(P). This node is then turned into a PP
	in a subsequent step (p.\,43--44).
\ea
Wallace saw Gromit in the kitchen.
\z
While this approach is a transformation"=based approach, the kind of transformation here is very idiosyncratic and incompatible with other
variants of the theory. In particular, the modification of constituents contradicts the assumption of Structure Preservation\is{Structure Preservation}
when applying transformations as well as the \emph{No Tampering Condition}\is{No Tampering Condition (NTC)} of \citet{Chomsky2008a}. 
Furthermore, the conditions under which an incomplete string such as \emph{Wallace saw Gromit in} forms a constituent are not
entirely clear.
%
} 
This will be briefly illustrated in what follows.
In Section~\ref{Abschnitt-GB-CP-IP-System-Englisch}, we encountered the following analysis of English interrogatives:
\ea
{}[\sub{CP} What$_i$ [\sub{C$'$} will$_k$ [\sub{IP} Ann [\sub{I$'$} \_$_k$ [\sub{VP} read \_$_i$]]]]].
\z
This structure is derived from (\mex{1}a) by two transformations (two applications of \movea):
\eal
\ex[]{
Ann will read what?
}
\ex[*]{
Will Ann read what
}
\zl
The first transformation creates the order in (\mex{0}b) from (\mex{0}a), and the second creates (\mex{-1}) from 
(\mex{0}b).

When a hearer processes the sentence in (\mex{-1}), he begins to build structure as soon as he hears the first word.
Transformations can, however, only be carried out when the entire utterance has been heard.
One can, of course, assume that hearers process surface structures. However, since  -- as we have seen -- they begin to access semantic knowledge
early into an utterance, this begs the question of what we need a deep structure for at all.

In analyses such as those of (\mex{-1}), deep structure is superfluous since the relevant information can be reconstructed
from the traces. Corresponding variants of GB have been proposed in the literature (see page~\pageref{Seite-Representationelle-GB}).
They are compatible with the requirement of being surface"=oriented.
 Chomsky (\citeyear[\page 181]{Chomsky81a}; \citeyear[\page 49]{Chomsky86b}) and
\citet[\page 59--60]{LS92a-u} propose analyses where traces can be deleted. In these analyses,
the deep structure cannot be directly reconstructed from the surface structure and one requires transformations
in order to relate the two.
If we assume that transformations are applied `online' during the analysis of utterances, then this would mean that the
hearer would have to keep a structure derived from previously heard material as well as a list of possible transformations
during processing in his working memory. In constraint"=based grammars\is{constraint"=based grammar}, entertaining hypotheses about potential upcoming transformation steps is not
necessary since there is only a single surface structure that is processed directly.
At present, it is still unclear whether it is actually possible to distinguish between these models empirically.
But for Minimalist models with a large number of movements (see Figure~\ref{Abbildung-Remnant-Movement-Satzstruktur}
on page~\pageref{Abbildung-Remnant-Movement-Satzstruktur}, for example), it should be clear that
they are unrealistic since storage space is required to manage the hypotheses regarding such
movements and we know that such short-term memory is very limited in humans.

\addlines
\citet[\page 27]{FC96a-u} assume that a transformation"=based competence grammar yields a grammar with pre"=compiled
rules or rather templates that is then used for parsing.
Therefore, theorems derived from UG are used for parsing and not axioms of UG directly.
\citet{Johnson89a} also suggests a parsing system that applies constraints from different sub"=theories of GB as early as possible.
This means that while he does assume the levels of representation D"=structure\is{D"=structure}, S"=structure\is{S"=structure}, LF and PF, he specifies the relevant
constraints (\xbart, Theta"=Theory\is{theta-theory@$\theta$-Theory}, Case Theory, \ldots) as logical
conditions that can be reorganized, then be evaluated in a different but logically
equivalent order and be used for structure building.%
\footnote{%
\citet[Section~15.7]{Stabler92a-u} also considers a constraint"=based\is{constraint"=based grammar} view, but arrives at the conclusion that parsing and other linguistic
tasks should use the structural levels of the competence theory. This would again pose problems for the DTC.%
}
\citet[\page 6]{Chomsky2007a} also compares human parsing to working through a proof, where each step of the proof can be carried out in different
orders. This view does not assume the psychological reality of levels of grammatical representation when processing language, but simply assumes
that principles and structures play a role when it comes to language acquisition\is{acquisition}. 
As we have seen, the question of whether we need UG to explain language acquisition was not yet decided in favor of UG"=based approaches.
Instead, all available evidence seems to point in the opposite direction. However, even if innate linguistic knowledge does exist, the
question arises as to why one would want to represent this as several structures linked via transformations when it is clear that these do not play
a role for humans (especially language learners) when processing language.
Approaches that can represent this knowledge using fewer technical means, \eg without
transformations, are therefore preferable. For more on this point, see \citew[\page 615]{Kuhn2007a}.

The requirement for constraint"=based grammars is supported by incremental processing and also by
the ability to deduce what will follow from previously heard material. \cite{Stabler91a} has pointed
out that \citeauthor{Steedman89b-u}'s \citeyearpar{Steedman89b-u} argumentation with regard to incrementally
processable grammars is incorrect, and instead argues for maintaining a modular view of
grammar. Stabler has developed a constraint"=based grammar where syntactic and semantic
knowledge can be accessed at any time. He formulates both syntactic structures and the semantic
representations attached to them as conjoined constraints and then presents a processing system
that processes structures based on the availability of parts of syntactic and semantic
knowledge. Stabler rejects models of performance that assume that one must first apply all syntactic
constraints before the semantic ones can be applied. If one abandons this strict view of modularity,
then we arrive at something like (\mex{1}):

\ea
(Syn$_1$ $\wedge$ Syn$_2$ $\wedge$ \ldots{} $\wedge$ Syn$_n$) $\wedge$ (Sem$_1$ $\wedge$ Sem$_2$ $\wedge$ \ldots{} $\wedge$ Sem$_n$)
\z
Syn$_1$--Syn$_n$ stand for syntactic rules or constraints and Sem$_1$--Sem$_n$ stand for semantic rules or constraints.
If one so desires, the expressions in brackets can be referred to as modules. Since it is possible
to randomly reorder conjoined expressions, one can imagine performance models that first apply some
rules from the syntax module and then, when enough information is present, respective rules from the
semantic module. The order of processing could therefore be as in (\mex{1}), for example: 
\ea
Syn$_2$ $\wedge$ Sem$_1$ $\wedge$ Syn$_1$ $\wedge$ \ldots{} $\wedge$ Syn$_n$ $\wedge$ Sem$_2$ $\wedge$ \ldots{} $\wedge$ Sem$_n$
\z

\noindent
If one subscribes to this view of modularity\is{module}, then theories such as HPSG or CxG also have a modular structure.
In the representation assumed in the HPSG variant of \citet{ps} and Sign-Based CxG (see Section~\ref{sec-SbCxG}),
the value of \textsc{syn} would correspond to the syntax module, the value of \textsc{sem} to the semantic module and the value of \textsc{phon}  to the phonology module. If one were to
remove the respective other parts of the lexical entries/dominance schemata, then one would
 be left with the part of the theory corresponding exactly to the level of representation in question.\footnote{%
  In current theories in the Minimalist Program, an increasing amount of morphological, syntactic, semantic and 
  information"=structural information is being included in analyses (see Section~\ref{Abschnitt-MP-funktionale-Projektionen}).
  While there are suggestions for using feature"=value pairs \citep[\page 290--291]{SE2002a}, a strict structuring of information
  as in GPSG\indexgpsg, LFG\indexlfg, HPSG\indexhpsg, CxG\indexcxg and variants of
  CG\indexcg and TAG\indextag is not present. This means that there are the levels for syntax, Phonological Form and Logical Form,
  but the information relevant for these levels is an unstructured part of syntax, smeared all over
  syntactic trees.
} 
\citew{Jackendoff2000a} argues for this form of modularity with the relevant interfaces between the modules for phonology, syntax, semantics and further modules from other areas of cognition. 
Exactly what there is to be gained from assuming these modules and how these could be proved empirically remains somewhat unclear to me. For skepticism with regard to the very concept of modules, see
\citew[\page 22, 27]{Jackendoff2000a}. For more on interfaces and modularization in theories such as LFG\indexlfg and HPSG\indexhpsg, see \citew{Kuhn2007a}.

Furthermore, \citet[\page 53--54]{SW2015a} argue that listeners often leave semantic
interpretation underspecified until enough information is present either in the utterance itself or
 the context. They do not commit to a certain reading early and run into garden paths or backtrack
to other readings. This is modeled appropriately by theories that use a variant of underspecified
semantics. For a concrete example of underspecification in semantics see Section~\ref{sec-MRS-wieder}.

In conclusion, we can say that surface"=oriented, model"=theoretic and strongly lexicalist
grammatical theories such as CG, LFG, GPSG, HPSG, CxG and the corresponding GB/MP variants (paired with appropriate semantic representations) can plausibly be combined with processing
models, while this is not the case for the overwhelming majority of GB/MP theories.
\is{competence|)}\is{performance|)}


%      <!-- Local IspellDict: en_US-w_accents -->
