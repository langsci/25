%% -*- coding:utf-8 -*-
\chapter{Conclusion}
\label{chap-conclusion}

The analyses discussed in this book show a number of similarities. All frameworks use complex
categories to describe linguistic objects. This is most obvious for GPSG\indexgpsg,
LFG\indexlfg, HPSG\indexhpsg, CxG\indexcxg and FTAG\indextag, however, GB/Minimalism and Categorial Grammar also talk about NPs in third person singular and the relevant features for part of
speech, person and number form part of a complex category. In GB, there are the feature N and V with binary values \citep[\page 199]{Chomsky70a}, \citet[\page
119]{Stabler92a-u} formalizes \emph{Barriers} with feature"=value pairs and \citet[\page
290--291]{SE2002a} argue for the use of feature"=value pairs in a Minimalist theory\indexmp. Also, see \citet[\page]{Veenstra98a} for a constraint"=based\is{constraint"=based grammar|(} formalization
of a Minimalist analysis using typed feature descriptions. Dependency Grammar dialects like
Hellwig's Dependency Unification Grammar\is{Dependency Unification Grammar (DUG)} also use feature"=value
pairs \citep[\page 612]{Hellwig2003a}.

Furthermore, there is a consensus in all current frameworks (with the exception of Construction
Grammar and Dependency Grammar) about how the sentence structure of German should
be analyzed: German is an SOV and V2 language. Clauses with verb"=initial order resemble verb"=final ones in terms of structure. The finite verb is
either moved (GB) or stands in a relation to an element in verb"=final position (HPSG). Verb"=second
clauses consist of  verb"=initial clauses out of which one constituent has been extracted. It is also possible to see some convergence with regard to the analysis of the passive: some ideas originally formulated
by \citet{Haider84b,Haider85b,Haider86} in the framework of GB have been adopted by HPSG. Some
variants of Construction Grammar also make use of a specially marked `designated
argument'\is{argument!designated} \citep[\page55--57]{MR2001a}.

If we consider new developments in the individual frameworks, it becomes clear that the nature of the proposed analyses can sometimes differ drastically.
Whereas CG, LFG, HPSG and CxG are surface"=oriented, sometimes very abstract structures are assumed in Minimalism and in some cases, one tries to trace all languages back
to a common base structure (Universal Base Hypothesis\is{Universal Base Hypothesis}).\footnote{%
  It should be noted that there are currently many subvariants and individual opinions in the Minimalist community so that it
  is only possible -- as with CxG -- to talk about tendencies.%
} This kind of approach only makes sense if one assumes that there is innate linguistic knowledge about this base structure
common to all languages as well as about the operations necessary to derive the surface structures.
As was shown in Chapter~\ref{chap-innateness}, all arguments for the assumption of innate linguistic knowledge are either not tenable
or controversial at the very least.
The acquisition of linguistic abilities can to a large extent receive an input"=based explanation
(Section~\ref{Abschnitt-UDOP}, Section~\ref{Abschnitt-musterbasiert} and
Section~\ref{Abschnitt-Selektionsbasierter-Spracherwerb}). Not all questions about acquisition have
been settled once and for all, but input"=based approaches are at least plausible enough for one to be very cautious about any assumption of innate linguistic knowledge.
% \citep[\page 4]{Chomsky2007a}

Models such as LFG, CG, HPSG, CxG and TAG are compatible with performance data, something that is
not true of certain transformation"=based approaches, which are viewed as theories of competence
that do not make any claims about performance. In MGG, it is assumed that there are other mechanisms
for working with linguistic knowledge, for example, mechanisms that combine `chunks' (fragments of linguistic material). If one wishes to make these assumptions,
then it is necessary to explain how chunks and the processing of chunks are acquired and not how a complex system of transformations and transformation"=comparing
constraints is acquired. This means that the problem of language acquisition would be a very different one. If one assumes a chunk"=based approach, then the innate
knowledge about a universal transformational base would only be used to derive a surface"=oriented grammar. This then poses the question of what exactly the evidence
for transformations in a competence grammar is and if it would not be preferable to simply assume that the competence grammar is of the kind assumed by LFG, CG, HPSG,
CxG or TAG. One can therefore conclude that constraint"=based\is{constraint"=based grammar} analyses and the kind of
transformational approaches that allow a constraint"=based reformulation are the only approaches that are compatible with the current facts, whereas all other analyses require additional assumptions.

A number of works in Minimalism differ from those in other frameworks in that they assume structures (sometimes also invisible structures) that can only be motivated
by evidence from other languages. This can streamline the entire apparatus for deriving
different structures, but the overall costs of the approach are not reduced: some amount of the
cost is just transferred to the UG component. The abstract grammars that result cannot be learned from the input.

One can take from this discussion that only constraint"=based, surface"=oriented models are adequate
and explanatory: they are also compatible with psycholinguistic facts and plausible from the point of view of acquisition.

If we now compare these approaches, we see that a number of analyses can be translated into one
another. LFG (and some variants of CxG and DG) differ from all other theories in that grammatical
functions\is{grammatical function} such as subject and object are primitives of the theory. If one does not want this, then it is possible to replace these labels with Argument1, Argument2,
etc. The numbering of arguments would correspond to their relative obliqueness. LFG would then move
closer to HPSG. Alternatively, one could mark arguments in HPSG and CxG with regard to their grammatical function additionally. This is what is done for the analysis of the passive (\textsc{designated argument}).

% Word Grammar auch Hudson90
LFG, HPSG, CxG and variants of Categorial Grammar \citep{MCKRZ89a-u,Briscoe2000a,Villavicencio2002a}
possess means for the hierarchical organization of knowledge\is{inheritance}, which is important for capturing generalizations.
It is, of course, possible to expand any other framework in this way, but this has never been done
explicitly, except in computer implementations and inheritance hierarchies do not play an active role in theorizing in the other frameworks.

In HPSG and CxG, roots, stems, words, morphological and syntactic rules are all objects that can be
described with the same means. This then allows one to make generalizations that affect very
different objects (see Chapter~\ref{Abschnitt-UG-mit-Hierarchie}).   
In LFG, c"=structures are viewed as something fundamentally different, which is why this kind of generalization is not possible. In cross"=linguistic
work, there is an attempt to capture similarities in the f"=structure, the c"=structure is less important and is not even discussed in a number of
works. Furthermore, its implementation from language to language can differ enormously. For this reason, my personal preference is for frameworks
that describe all linguistic objects using the same means, that is, HPSG and CxG. Formally, nothing stands in the way of a description of the c"=structure
of an LFG grammar using feature"=value pairs so that in years to come there could be even more
convergence between the theories. For hybrid forms of HPSG and LFG, see \citew{AW98a} and \citew{HH2004a-u}, for example.

If one compares CxG and HPSG, it becomes apparent that the degree of formalization in CxG works is relatively low and a number of questions remain
unanswered. The more formal approaches in CxG (with the exception of Fluid Construction Grammar) are variants of HPSG. There are relatively few precisely worked"=out analyses in Construction Grammar
and no description of German that would be comparable to the other approaches presented in this
book. To be fair, it must be said that Construction Grammar
is the youngest of the theories discussed here. Its most important contributions to linguistic theory have been integrated into frameworks such as HPSG and LFG.

The theories of the future will be a fusion of surface"=oriented, constraint"=based and
model"=theoretic approaches like CG\indexcg, LFG\indexlfg, HPSG\indexhpsg,
Construction Grammar\indexcxg, equivalent variants of TAG\indextag and GB/Minimalist\indexgb\indexmp approaches that will be reformulated as constraint"=based\is{constraint"=based grammar|)}.
(Variants of) Minimalism and (variants of) Construction Grammar are the most widely adopted approaches at present. I actually suspect the truth to lie somewhere
in the middle. The linguistics of the future will be data"=oriented. Introspection\is{introspection} as the sole method of data collection
has proven unreliable \citep{Mueller2007c,MM2009a} and is being increasingly complemented by experimental and corpus"=based\is{corpus linguistics} analyses.\todostefan{AL 16.05.2010: zur grammatikalität finde ich ja den aufsatz 'grammar without grammaticality' von sampson im CLLt gut (und auch die anderen Artikel im selben heft, die sich darauf beziehen)}

Statistical information and statistical processes play a very important role in machine translation and are becoming more important
for linguistics in the narrow sense
\citep{Abney96a}. We\todostefan{AL 16.05.2010: müsste man hier nicht unbedingt Bod/Hay/Jannedy Probabilistic linguistics zitieren? }
have seen that statistical information is important in the acquisition process and Abney discusses cases of other
areas of language such as language change, parsing preferences and gradience with grammaticality judgments.
%Modelle wie GB \citep{FC94a}, TAG \citep{Resnik92a}, HPSG \citep{Brew95a}, LFG \citep{HK2007a-u}
%und CG \citep*{OB97a,CHS2002a-u} werden auch mit statistischen Elementen kombiniert.
Following a heavy focus on statistical procedures, there is now a transition to hybrid forms in computational linguistics,\footnote{%
See \citew{KP2007a} and \citew{Kaufmann2009a-u} for the combination of a speech recognizer with a HPSG grammar.
}
since it has been noticed that it is not possible to exceed certain levels of quality with statistical methods alone \citep{Steedman2011a,Church2011a,Kay2011a}. 
The same holds here as above: the truth is somewhere in between, that is, in combined systems. In order to have something to combine, the relevant linguistic theories first
need to be developed. As Manfred Pinkal said: ``It is not possible to build systems that understand language without understanding language.''


%      <!-- Local IspellDict: en_US-w_accents -->
